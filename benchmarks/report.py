#!/usr/bin/env python3
"""
Aggregate benchmark CSV results into a compact Markdown report.

Example:
    python -m benchmarks.report --results results/cmap.csv results/movingai.csv --output reports/summary.md
"""
from __future__ import annotations

import argparse
import ast
import csv
from pathlib import Path
from statistics import mean
from typing import Dict, List


def parse_csv(path: Path) -> List[Dict[str, str]]:
    with path.open("r", newline="", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        return list(reader)


def summarise(rows: List[Dict[str, str]]) -> Dict[str, float]:
    speedups = [float(r["cpu_time"]) / max(float(r["gpu_time"]), 1e-9) for r in rows]
    maes = [float(r["mae"]) for r in rows]
    path_ratios = [float(r["path_ratio"]) for r in rows]
    return {
        "cases": len(rows),
        "speedup_mean": mean(speedups),
        "mae_mean": mean(maes),
        "path_ratio_mean": mean(path_ratios),
    }


def infer_dataset_name(rows: List[Dict[str, str]]) -> str:
    if not rows:
        return "unknown"
    meta = rows[0]["metadata"]
    try:
        decoded = ast.literal_eval(meta)
        if isinstance(decoded, dict) and "dataset" in decoded:
            return str(decoded["dataset"])
    except Exception:
        pass
    return "unknown"


def write_markdown(path: Path, summaries: Dict[str, Dict[str, float]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as handle:
        handle.write("# Benchmark Summary\n\n")
        handle.write("| Dataset | Cases | Mean Speedup | Mean MAE | Mean Path Ratio |\n")
        handle.write("|---------|-------|--------------|----------|------------------|\n")
        for name, stats in summaries.items():
            handle.write(
                f"| {name} | {int(stats['cases'])} | {stats['speedup_mean']:.2f}x | "
                f"{stats['mae_mean']:.4f} | {stats['path_ratio_mean']:.4f} |\n"
            )
        handle.write("\nGenerated by `benchmarks.report`.\n")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--results", nargs="+", type=Path, required=True, help="CSV result files from run_suite")
    parser.add_argument("--output", type=Path, required=True, help="Destination Markdown report")
    return parser


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()
    summaries: Dict[str, Dict[str, float]] = {}
    for csv_path in args.results:
        rows = parse_csv(csv_path)
        dataset_name = infer_dataset_name(rows)
        stats = summarise(rows)
        summaries[f"{dataset_name} ({csv_path.stem})"] = stats
    write_markdown(args.output, summaries)
    print(f"[report] Wrote summary to {args.output}")


if __name__ == "__main__":
    main()


